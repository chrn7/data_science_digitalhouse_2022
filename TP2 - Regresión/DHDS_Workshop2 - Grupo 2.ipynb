{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7a1836",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba31c21",
   "metadata": {},
   "source": [
    "# <u>Workshop 2 - Grupo 2</u>\n",
    "\n",
    "## DS Consultores - _INTEGRANTES:_\n",
    "> ####      • CANGUEIRO, María Mercedes\n",
    "> ####      • CHAMUT, Diego\n",
    "> ####      • FERNÁNDEZ, Andrea\n",
    "> ####      • GIORGETTI, María Gimena\n",
    "> ####      • LLANOS, Santiago Francisco\n",
    "> ####      • RIVAS NIETO, Christian Javier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73a9f7",
   "metadata": {},
   "source": [
    "## Modelos de ML para predecir el Precio Total en usd de una Propiedad\n",
    "\n",
    "\n",
    "---\n",
    "<img src=\"https://www.nocnok.com/hs-fs/hubfs/Documentos.jpg?width=1254&name=Documentos.jpg\"\n",
    "align=\"center\"  width=50%/></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e360851",
   "metadata": {},
   "source": [
    "<a id=\"section_TOC\"></a>\n",
    "## Tabla de Contenidos\n",
    "\n",
    "[1. Introducción](#section_intro)\n",
    "\n",
    "[2. Dataset](#section_dataset)\n",
    "\n",
    "$\\hspace{.5cm}$[2.1. Importación de Datos](#section_2.1)\n",
    "\n",
    "$\\hspace{.5cm}$[2.2. Verificación de valores Nulos](#section_2.2)\n",
    "\n",
    "$\\hspace{.5cm}$[2.3. Imputación de valores Nulos](#section_2.3)\n",
    "\n",
    "$\\hspace{.5cm}$[2.4. Limpieza de Outliers](#section_2.4)\n",
    "\n",
    "[3. Feature Engineering (Modelado)](#section_modelado)\n",
    "\n",
    "[4. Regresión Lineal Simple](#section_lineal_simple)\n",
    "\n",
    "$\\hspace{.5cm}$[4.1. Regresión Lineal Simple Superficie Total](#section_4.1)\n",
    "\n",
    "$\\hspace{.5cm}$[4.2. Regresión Lineal Simple Ambientes](#section_4.2)\n",
    "\n",
    "$\\hspace{.5cm}$[4.3. Regresión Lineal Simple Estrenar](#section_4.3)\n",
    "\n",
    "$\\hspace{.5cm}$[4.4. Regresión Lineal Simple Cochera](#section_4.4)\n",
    "\n",
    "$\\hspace{.5cm}$[4.5. Regresión Lineal Simple Superficie Cubierta](#section_4.5)\n",
    "\n",
    "[5. Regresión Lineal Multiple](#section_lineal_multiple)\n",
    "\n",
    "$\\hspace{.5cm}$[5.1. Regresión Lineal Multiple SIN Regularizacion](#section_5.1)\n",
    "\n",
    "$\\hspace{.5cm}$[5.2. Regresión Lineal Multiple CON Regularizacion](#section_5.2)\n",
    "\n",
    "[6. Regresión Lineal Ridge](#section_lineal_ridge)\n",
    "  \n",
    "[7. Regresión Lineal Lasso](#section_lineal_lasso)\n",
    "\n",
    "[8. Regresión Elastic Net](#section_elastic_net)\n",
    "\n",
    "[9. Conclusiones ](#section_conclusiones)\n",
    "\n",
    "$\\hspace{.5cm}$[9.1. Comparación Numérica de los Distintos Modelos](#section_9.1)\n",
    "\n",
    "$\\hspace{.5cm}$[9.2. Comparación Gráfica de los Distintos Modelos](#section_9.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3833d",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a>\n",
    "## 1. Introducción\n",
    "\n",
    "[volver a TOC](#section_TOC)\n",
    "\n",
    "\n",
    "En este segundo workshop del curso de Data Science de Digital House, nos enfocaremos en desarrollar un modelo de regresión que permita predecir el precio total en USD de una propiedad, para que, en un futuro, la inmobiliaria Properati pueda utilizarlo como tasador automático en las próximas propiedades que sean comercializadas.\n",
    "Realizaremos distintos modelos para poder comparar los resultados obtenidos y arribar al óptimo.<br>\n",
    "Se plantearán diversas estrategias para abordar fallas y/o faltantes de información que perjudiquen el resultado del modelo, en vistas de lograr el objetivo mencionado anteriormente. <br>\n",
    "Todo esto lo haremos mediante la aplicación de los conocimientos adquiridos hasta el momento a lo largo del cursado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61c5e9",
   "metadata": {},
   "source": [
    "A continuación, efectuamos la **importación** de aquellas **librerías** que utilizaremos a lo largo de todo el trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcdca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos pandas, numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857c5ec",
   "metadata": {},
   "source": [
    "<a id=\"section_dataset\"></a> \n",
    "## 2. Dataset\n",
    "\n",
    "[volver a TOC](#section_TOC)\n",
    "\n",
    "https://drive.google.com/file/d/0BzVrTKc02N8qNUdDSExBQlFTNlU/view?resourcekey=0-4m-28Uq6kWRDBrt2NXFbNQ\n",
    "\n",
    "El dataset contiene información sobre todas las propiedades georeferenciadas de la base de datos de la empresa. \n",
    "La información que cada propiedad incluye es la siguiente:\n",
    "\n",
    "**operation**: sell, rent, **property_type**: house, apartment, ph, **place_name**, **place_with_parent_names**, **country_name**, **state_name**, **geonames_id** (si está disponible), **lat-lon**, **price** (precio original del aviso), **currency**: ARS, USD, **price_aprox_local_currency**: ARS, **price_aprox_usd**, **surface_total_in_m2**, **surface_covered_in_m2**, **price_usd_per_m2**, **price_per_m2**, **floor**: (si corresponde), **rooms, expenses, properati_url, description, title, image_thumbnail**\n",
    "\n",
    "Trabajamos con el dataset limpio obtenido luego de todo el proceso de ETL realizado en el Workshop 1. <br>\n",
    "\n",
    "Adcionalmente, sólo nos enfocaremos en los registros de CABA y GBA (excluyendo los outliers), debido a que estas regiones contienen más del 80% de los registros del dataset original.\n",
    "\n",
    "Realizaremos en esta etapa los pasos necesarios para: <br>\n",
    "* Agregar las columnas que consideramos relevantes para nuestro análisis\n",
    "* Quitar/imputar valores nulos que perjudiquen el modelo \n",
    "* Quitar outliers que no hayan sido limpiados en el trabajo anterior, utilizando nuevos métodos (como DBScan)\n",
    "* Definir el dataset más ordenado posible, para luego entrenar nuestros modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164589e",
   "metadata": {},
   "source": [
    "<a id=\"section_2.1\"></a> \n",
    "### 2.1. Importación de Datos\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd3ca7",
   "metadata": {},
   "source": [
    "Importamos el dataset limpio, que resultó del trabajo integrador anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti = pd.read_csv('data/properatti_workshop2.csv')\n",
    "df_properatti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218d3e6",
   "metadata": {},
   "source": [
    "#### Creación de un df que contenga información georeferencial (GeoPandas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695832ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos df_prop que lo usaremos más adelante para crear algunos mapas:\n",
    "df_prop = df_properatti[['place_name', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380db9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660011849708,
     "user": {
      "displayName": "Diego Chamut",
      "userId": "04798294237895833971"
     },
     "user_tz": 180
    },
    "id": "dc34ae94",
    "outputId": "f0304b1c-ceb6-4b2b-84f2-0f1115831173"
   },
   "outputs": [],
   "source": [
    "#Trabajamos en otro df que creamos sin valores nulos en la columna 'lat':\n",
    "data_lat_notnull = df_prop.loc[df_prop.lat.notnull()]\n",
    "data_lat_notnull.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1fc101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1660011849708,
     "user": {
      "displayName": "Diego Chamut",
      "userId": "04798294237895833971"
     },
     "user_tz": 180
    },
    "id": "c3868a67",
    "outputId": "838a232a-6151-4b3f-eba9-0d4eb44af147"
   },
   "outputs": [],
   "source": [
    "#Calculamos la mediana de las latitudes por place_name:\n",
    "data_coordenadas = data_lat_notnull.groupby('place_name')[['lat','lon']].median()\n",
    "data_coordenadas.reset_index(inplace = True)\n",
    "data_coordenadas.columns = ('place_name','lat_median','lon_median')\n",
    "data_coordenadas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc042b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1660011850131,
     "user": {
      "displayName": "Diego Chamut",
      "userId": "04798294237895833971"
     },
     "user_tz": 180
    },
    "id": "e1b97e30",
    "outputId": "c9c0abcf-2bda-4979-e628-ca50aeeef0fc"
   },
   "outputs": [],
   "source": [
    "#Agregamos nuevas columnas con las medianas de lat y lon por place_name:\n",
    "df_prop = pd.merge(df_prop, data_coordenadas, on='place_name', how = 'left')\n",
    "df_prop.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe01cb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1660011850131,
     "user": {
      "displayName": "Diego Chamut",
      "userId": "04798294237895833971"
     },
     "user_tz": 180
    },
    "id": "6c71fbdd"
   },
   "outputs": [],
   "source": [
    "#Rellenamos valores nulos de lon y lat del df mayor, con las medianas:\n",
    "df_prop.lat.fillna(df_prop.lat_median, inplace = True)\n",
    "\n",
    "df_prop.lon.fillna(df_prop.lon_median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7cf9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1660011850132,
     "user": {
      "displayName": "Diego Chamut",
      "userId": "04798294237895833971"
     },
     "user_tz": 180
    },
    "id": "c5e17fa5",
    "outputId": "2621ce17-4d06-4946-f725-e7eadaf4af00"
   },
   "outputs": [],
   "source": [
    "#Los datos que quedan vacíos es por que no hay ningún dato de geoposicionamiento en los datos del df:\n",
    "df_prop.loc[df_prop.lat.isnull()].head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los valores que no tienen datos de latitud, que son los mismos que no tienen longitud:\n",
    "df_prop.dropna(inplace = True, subset =['lat_median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcad10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "elapsed": 4829,
     "status": "ok",
     "timestamp": 1660011854955,
     "user": {
      "displayName": "Diego Chamut",
      "userId": "04798294237895833971"
     },
     "user_tz": 180
    },
    "id": "ae90f454",
    "outputId": "0cb42bd3-bab7-49f5-c080-e0f2dee086b8"
   },
   "outputs": [],
   "source": [
    "#Transformamos el df en geodf\n",
    "geo_properatti = gpd.GeoDataFrame(df_prop,geometry=gpd.points_from_xy(df_prop.lon, df_prop.lat))\n",
    "geo_properatti.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28892c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregamos un archivo shp para poder ver los departamentos de Argentina en forma de poligonos\n",
    "departamentos_arg = gpd.read_file('data/departamento.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ad96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "departamentos_arg.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2ed04",
   "metadata": {},
   "source": [
    "#### Creación de Diccionario para generar nueva columna:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175eaed",
   "metadata": {},
   "source": [
    "El objetivo es filtrar luego sólo las propiedades que pertenezcan a CABA y GBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2bcdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CABA_or_GBA_dict = {\n",
    "    'Capital Federal'             : 'CABA',\n",
    "    'Bs.As. G.B.A. Zona Sur'      : 'GBA',\n",
    "    'Buenos Aires Costa Atlántica': 'Buenos Aires',\n",
    "    'Bs.As. G.B.A. Zona Norte'    : 'GBA',\n",
    "    'Bs.As. G.B.A. Zona Oeste'    : 'GBA',\n",
    "    'Buenos Aires Interior'       : 'Buenos Aires'\n",
    "}\n",
    "#Creamos una columna nueva llamada 'CABA/GBA' con los valores que tiene la columna 'state_name'. Luego reemplazamos los valores de la columna 'CABA/GBA' por los del Diccionario:\n",
    "df_properatti['CABA_or_GBA']= df_properatti['state_name']\n",
    "df_properatti.replace({'CABA_or_GBA':CABA_or_GBA_dict}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d68f39",
   "metadata": {},
   "source": [
    "Chequeamos que efectivamente se haya creado la columna 'CABA/GBA' en el dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89077ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b053b2a3",
   "metadata": {},
   "source": [
    "#### Selección de columnas relevantes para el análisis\n",
    "\n",
    "Nos quedamos sólo con algunas columnas del Dataframe, que consideramos van a ser de utilidad luego en los modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti=df_properatti[['property_type','CABA_or_GBA','place_name','balcon','terraza_patio','estrenar','cochera','ambientes','floor','ARS_to_USD_corregido','price_usd_per_m2','surface_total_in_m2', 'surface_covered_in_m2']]\n",
    "\n",
    "df_properatti.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab1cd8",
   "metadata": {},
   "source": [
    "<a id=\"section_2.2\"></a> \n",
    "### 2.2. Verificación de Valores Nulos\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349264bb",
   "metadata": {},
   "source": [
    "Vamos a corroborar si el Dataframe tiene valores nulos o Nulls (NaN) en alguna de sus columnas. De ser así, los registros que tengan Nulls habrá que eliminarlos o imputarlos de alguna forma, para que no perjudiquen a los modelos que utilizaremos luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ef500",
   "metadata": {},
   "source": [
    "Como vemos, las columnas _estrenar_, _cochera_, _ambientes_, _floor_ y _surface_covered_in_m2_ son las únicas que tiene valores Nulls (NaN). Sin embargo, las columnas _estrenar_ y _cochera_ son binarias: 1 si ES \"a estrenar\" o si TIENE \"cochera\", y 0 en caso contrario. Por cómo fueron creadas, los Nulls en las mismas en realidad deberían ser ceros. <br>\n",
    "Por lo tanto, procedemos a efectuar dicho reemplazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f64453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.estrenar.fillna(value=0 , inplace = True)\n",
    "df_properatti.cochera.fillna(value=0 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0656c",
   "metadata": {},
   "source": [
    "La columna _floor_ vamos a descartarla, por tratarse en su mayoría de registros nulos (aprox. 93%), y considerar que no aporta información tan relevante como predictora. <br>\n",
    "Por el contrario, vamos a enfocarnos en completar los datos de las columnas **superficie cubierta** (_surface_covered_in_m2_) y **ambientes** (_ambientes_), ya que consideramos que estas sí pueden ser de mayor utilidad como variables predictoras.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c329a75",
   "metadata": {},
   "source": [
    "<a id=\"section_2.3\"></a> \n",
    "### 2.3. Imputación de Valores Nulos\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc8f37e",
   "metadata": {},
   "source": [
    "#### Imputación de valores columnas _ambientes_ y _surface_covered_in_m2_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2a1bb",
   "metadata": {},
   "source": [
    "Vamos a imputar los valores faltantes en la columna _ambientes_ utilizando información de la columna _surface_covered_in_m2_. Para ello, primero tendremos que completar esta última, utilizando información de la columna _surface_total_in_m2_.<br>\n",
    "Veamos primero la columna **_ambientes_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amb = round(df_properatti.groupby('property_type')['ambientes'].describe(),2)\n",
    "print('                     AMBIENTES')\n",
    "df_amb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_amb.iloc[:,[1,5]], ((round(df_amb.iloc[:,1]/df_amb.iloc[:,5],2)-1)*100).rename('mean/median[%]')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626f437",
   "metadata": {},
   "source": [
    "Vemos en la tabla anterior datos \"curiosos\", probablemente erróneos, como _departamentos_ con _85 ambientes_. <br>\n",
    "\n",
    "Sin embargo, por lo pronto, verificamos que para todos los tipos de propiedad, la mediana se encuentra relativamente cerca de la media, ligeramente por debajo de la misma. La diferencia promedio entre estas dos magnitudes, para todos los tipos de propiedad, es de aprox. 15%. Esta diferencia existe, seguramente, debido a la presencia de outliers, aunque probablemente no sean tantos, ya que los valores no son tan distantes entre sí. <br>\n",
    "\n",
    "Tomaremos la **mediana** como valor de referencia a la hora de imputar, ya que además se trata de un número entero. <br>\n",
    "\n",
    "Veamos ahora la columna **superficie cubierta** (_surface_covered_in_m2_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e328d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sup_c = round(df_properatti.groupby('property_type')['surface_covered_in_m2'].describe(),2)\n",
    "print('                   SUPERFICIE CUBIERTA')\n",
    "df_sup_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_sup_c.iloc[:,[1,5]], ((round(df_sup_c.iloc[:,1]/df_sup_c.iloc[:,5],2)-1)*100).rename('mean/median[%]')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464433dd",
   "metadata": {},
   "source": [
    "En este caso, la diferencia entre los dos estadísticos es mayor, lo que indica mayor presencia de outliers (o valores más extremos), sobre todo para el tipo de propiedad **stores**. Vemos que, en promedio, en los primeros 3 tipos de propiedad, hay una diferencia de aprox. 24% (la media por encima de la mediana), pero en el caso de stores, esta diferencia asciende a 171%, elevando el promedio total significativamente. <br>\n",
    "Tendremos esto en cuenta a la hora de tratar de corregir posibles outliers y luego efectuar la imputación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2802c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos cantidad de nulos por tipo de propiedad:\n",
    "amb_nulls = df_properatti.ambientes.isnull().groupby(df_properatti['property_type']).sum()\n",
    "sup_c_nulls = df_properatti.surface_covered_in_m2.isnull().groupby(df_properatti['property_type']).sum()\n",
    "print('    CANT. DE NULOS POR TIPO DE PROPIEDAD')\n",
    "pd.concat([amb_nulls, sup_c_nulls], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98425356",
   "metadata": {},
   "source": [
    "Vemos que, tanto los **departamentos** como las **casas**, registran el mayor número de nulos para las dos columnas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos variables \n",
    "amb_nulls_PH = df_properatti.loc[(df_properatti['property_type']=='PH') & (df_properatti['ambientes'].isnull())]['ambientes']\n",
    "amb_nulls_apart = df_properatti.loc[(df_properatti['property_type']=='apartment') & (df_properatti['ambientes'].isnull())]['ambientes']\n",
    "amb_nulls_house = df_properatti.loc[(df_properatti['property_type']=='house') & (df_properatti['ambientes'].isnull())]['ambientes']\n",
    "amb_nulls_store = df_properatti.loc[(df_properatti['property_type']=='store') & (df_properatti['ambientes'].isnull())]['ambientes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a7d17",
   "metadata": {},
   "source": [
    "Debemos averiguar los valores que vamos a utilizar para imputar. Primero, debemos completar la columna _surface_covered_in_m2_. Vamos a imputar datos desde _surface_total_in_m2_.<br> \n",
    "Averiguamos qué relación hay entre estas dos columnas, para cada tipo de propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sup_summary = round(df_properatti.groupby('property_type')[['surface_covered_in_m2', 'surface_total_in_m2']].mean(),2)\n",
    "df_sup_summary\n",
    "print('     RELACIÓN ENTRE SUP. CUB. Y SUP. TOTAL POR TIPO DE PROPIEDAD')\n",
    "df_sup_summ = pd.concat([df_sup_summary, ((round(df_sup_summary.iloc[:,0]/df_sup_summary.iloc[:,1],2))*100).rename('surf. cov./surf. total[%]')], axis=1)\n",
    "df_sup_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d8371",
   "metadata": {},
   "source": [
    "Usaremos los valores de la **tercera columna** para imputar la sup. cubierta de cada tipo de propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e06d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputamos los nulos en SUPERFICIE CUBIERTA de cada tipo de propiedad:\n",
    "#PH:\n",
    "df_properatti.loc[(df_properatti['property_type']=='PH')&(df_properatti['surface_covered_in_m2'].isnull()), 'surface_covered_in_m2'] = \\\n",
    "                                                    df_properatti.surface_total_in_m2*(df_sup_summ.loc['PH','surf. cov./surf. total[%]']/100)\n",
    "#Departamentos:\n",
    "df_properatti.loc[(df_properatti['property_type']=='apartment')&(df_properatti['surface_covered_in_m2'].isnull()), 'surface_covered_in_m2'] = \\\n",
    "                                                    df_properatti.surface_total_in_m2*(df_sup_summ.loc['apartment','surf. cov./surf. total[%]']/100)\n",
    "\n",
    "#House:\n",
    "df_properatti.loc[(df_properatti['property_type']=='house')&(df_properatti['surface_covered_in_m2'].isnull()), 'surface_covered_in_m2'] = \\\n",
    "                                                    df_properatti.surface_total_in_m2*(df_sup_summ.loc['house','surf. cov./surf. total[%]']/100)\n",
    "\n",
    "#Store:\n",
    "df_properatti.loc[(df_properatti['property_type']=='store')&(df_properatti['surface_covered_in_m2'].isnull()), 'surface_covered_in_m2'] = \\\n",
    "                                                    df_properatti.surface_total_in_m2*(df_sup_summ.loc['store','surf. cov./surf. total[%]']/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos nuevamente cantidad de nulos de superficie cubierta, por tipo de propiedad, verificando que sean todos cero:\n",
    "amb_nulls = df_properatti.ambientes.isnull().groupby(df_properatti['property_type']).sum()\n",
    "sup_c_nulls = df_properatti.surface_covered_in_m2.isnull().groupby(df_properatti['property_type']).sum()\n",
    "print('    CANT. DE NULOS POR TIPO DE PROPIEDAD')\n",
    "pd.concat([amb_nulls, sup_c_nulls], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb1668",
   "metadata": {},
   "source": [
    "Con la tabla anterior, verificamos que, según lo que indica la segunda columna, ya no hay valores nulos para _surface_covered_in_m2_.<br>\n",
    "Ahora vamos a hacer lo mismo para la columna **_ambientes_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_amb = df_properatti.groupby(['property_type', 'ambientes'])['surface_covered_in_m2'].describe().loc[:,['count','min','25%','50%','mean','75%','max']]\n",
    "tabla_amb.columns=['count','min','25%','median(50%)','mean','75%','max']\n",
    "tabla_amb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba10c9",
   "metadata": {},
   "source": [
    "Podemos apreciar que hay inconsistencias en la progresión de la superficie cubierta en algunos casos. <br>\n",
    "Para poder establecer intervalos, es necesario que aumente en \"forma ascendente\". En algunos casos, se puede ver que la mediana de superficie cubierta disminuye, para mayor número de ambientes. <br>\n",
    "Esto no debería ocurrir, por lo que vamos a tratar de corregir esos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60302039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos valores extremos:\n",
    "df_properatti.drop(df_properatti[(df_properatti.property_type=='apartment')&(df_properatti.ambientes>=12)].index, inplace = True)\n",
    "df_properatti.drop(df_properatti[(df_properatti.property_type=='house')&(df_properatti.ambientes>=10)].index, inplace = True)\n",
    "df_properatti.drop(df_properatti[(df_properatti.property_type=='store')&(df_properatti.ambientes>=4)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volvemos a chequear la tabla anterior:\n",
    "tabla_amb = df_properatti.groupby(['property_type', 'ambientes'])['surface_covered_in_m2'].describe().loc[:,['count','min','25%','50%','mean','75%','max']]\n",
    "tabla_amb.columns=['count','min','25%','median(50%)','mean','75%','max']\n",
    "tabla_amb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos las variables por tipo de propiedad para llamarlas luego en la imputación:\n",
    "median_imput_factor = 1.1  \n",
    "df_tabla_amb = tabla_amb.reset_index()\n",
    "df_tabla_amb['lim_sup_interval_amb'] = df_tabla_amb['median(50%)'] * median_imput_factor\n",
    "\n",
    "#PH:\n",
    "int_amb_PH = list(df_tabla_amb['lim_sup_interval_amb'][df_tabla_amb.property_type == 'PH'])\n",
    "int_amb_PH.insert(0,0)\n",
    "nro_amb_PH = list(df_tabla_amb['ambientes'][df_tabla_amb.property_type == 'PH'])\n",
    "\n",
    "#Departamentos:\n",
    "int_amb_apartment = list(df_tabla_amb['lim_sup_interval_amb'][df_tabla_amb.property_type == 'apartment'])\n",
    "int_amb_apartment.insert(0,0)\n",
    "nro_amb_apartment = list(df_tabla_amb['ambientes'][df_tabla_amb.property_type == 'apartment'])\n",
    "\n",
    "#Casas. No podemos utilizar los valores de la tabla anterior, porque debido a la inconsistencia en la progresión, se verifica que en algunos\n",
    "#casos disminuye la superficie cubierta para un mayor nro. de ambientes. Por ese motivo, definimos el intervalo correspondiente:\n",
    "int_amb_house = [60,90,120,160,210,260,310,390,450,520,50000]\n",
    "int_amb_house.insert(0,0)\n",
    "nro_amb_house = list(np.arange(1,12))\n",
    "\n",
    "#Negocios:\n",
    "int_amb_store = list(df_tabla_amb['lim_sup_interval_amb'][df_tabla_amb.property_type == 'store'])\n",
    "int_amb_store.extend([300,400,20000])\n",
    "int_amb_store.insert(0,0)\n",
    "nro_amb_store = list(np.arange(1,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos variables \n",
    "sc_m2_nulls_PH = df_properatti.loc[(df_properatti['property_type']=='PH') & (df_properatti['ambientes'].isnull())]['surface_covered_in_m2']\n",
    "sc_m2_nulls_apartment = df_properatti.loc[(df_properatti['property_type']=='apartment') & (df_properatti['ambientes'].isnull())]['surface_covered_in_m2']\n",
    "sc_m2_nulls_house = df_properatti.loc[(df_properatti['property_type']=='house') & (df_properatti['ambientes'].isnull())]['surface_covered_in_m2']\n",
    "sc_m2_nulls_store = df_properatti.loc[(df_properatti['property_type']=='store') & (df_properatti['ambientes'].isnull())]['surface_covered_in_m2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29488a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos los intervalos con la cantidad de ambientes correspondientes según la superficie cubierta: \n",
    "cant_amb_PH = pd.cut(sc_m2_nulls_PH, int_amb_PH, labels=nro_amb_PH)\n",
    "cant_amb_apartment = pd.cut(sc_m2_nulls_apartment, int_amb_apartment, labels=nro_amb_apartment)\n",
    "cant_amb_house = pd.cut(sc_m2_nulls_house, int_amb_house, labels=nro_amb_house)\n",
    "cant_amb_store = pd.cut(sc_m2_nulls_store, int_amb_store, labels=nro_amb_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos nuevas columnas para guardar estos datos y luego imputar llamando a estos valores:\n",
    "df_properatti['cant_amb_PH'] = cant_amb_PH\n",
    "df_properatti['cant_amb_apartment'] = cant_amb_apartment\n",
    "df_properatti['cant_amb_house'] = cant_amb_house\n",
    "df_properatti['cant_amb_store'] = cant_amb_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciamos la cantidad de categorías (cant. de ambientes) que podrá tomar dicha columna:\n",
    "ambientes_categories = list(set(df_properatti.cant_amb_PH.cat.categories) | set(df_properatti.cant_amb_apartment.cat.categories) \n",
    "                            | set(df_properatti.cant_amb_house.cat.categories) | set(df_properatti.cant_amb_store.cat.categories))\n",
    "ambientes_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f251d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la columna ambientes como categórica, para cargarle los intervalos posibles definidos anteriormente:\n",
    "df_properatti.ambientes = pd.Categorical(df_properatti.ambientes, ordered=True)\n",
    "df_properatti.ambientes = df_properatti.ambientes.cat.set_categories(ambientes_categories, ordered=True)\n",
    "df_properatti.ambientes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputamos los valores según el tipo de propiedad:\n",
    "df_properatti.ambientes.fillna(df_properatti.cant_amb_PH, axis=0, inplace=True)\n",
    "df_properatti.ambientes.fillna(df_properatti.cant_amb_apartment, axis=0, inplace=True)\n",
    "df_properatti.ambientes.fillna(df_properatti.cant_amb_house, axis=0, inplace=True)\n",
    "df_properatti.ambientes.fillna(df_properatti.cant_amb_store, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7daa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos nuevamente cantidad de nulos por tipo de propiedad, verificando que sean todos cero:\n",
    "amb_nulls_2 = df_properatti.ambientes.isnull().groupby(df_properatti['property_type']).sum()\n",
    "sup_c_nulls_2 = df_properatti.surface_covered_in_m2.isnull().groupby(df_properatti['property_type']).sum()\n",
    "print('    CANT. DE NULOS POR TIPO DE PROPIEDAD')\n",
    "pd.concat([amb_nulls_2, sup_c_nulls_2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473e170",
   "metadata": {},
   "source": [
    "Verificamos que ya no hay valores nulos tampoco en la columna _ambientes_. Hemos **completado este proceso de imputación**.<br>\n",
    "Haremos la verificación final, antes de continuar a la siguiente etapa del análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisamos la forma actual del DataFrame para revisar cantidad de filas y columnas:\n",
    "df_properatti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las columnas creadas para este fin:\n",
    "df_properatti = df_properatti.drop(['cant_amb_PH', 'cant_amb_apartment', 'cant_amb_house', 'cant_amb_store'], axis=1)\n",
    "df_properatti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de55973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos las primeras filas para corroborar que las columnas se hayan eliminado correctamente:\n",
    "df_properatti.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14405f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4773189",
   "metadata": {},
   "source": [
    "La columna o variable _ambientes_ la habíamos definido como \"categórica\", a los efectos de realizar el proceso previo. Ahora la vamos a convertir en variable de tipo \"numérico\", para utilizarla luego en los modelos de regresión. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36290b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.ambientes = df_properatti.ambientes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2be23",
   "metadata": {},
   "source": [
    "Verificamos que figure como tipo _int32_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da28aec",
   "metadata": {},
   "source": [
    "<a id=\"section_2.4\"></a> \n",
    "### 2.4. Limpieza de Outliers\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30e0c3",
   "metadata": {},
   "source": [
    "Luego de imputar los valores vacíos de la columna _ambientes_ mediante su relación con la _superficie cubierta_, queremos controlar si existen otros valores poco representativos.<br>\n",
    "Por lo que analizaremos si aún existen valores extremos para cada tipo de propiedad (apartment, house, PH, store), e intentaremos corregirlos, como último paso previo a la utilización de los modelos de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'apartment'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean() \n",
    "#un par de datos extraños en los extremos pero es el tipo de propiedad más completo para trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'house'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean() \n",
    "#muchos datos vacíos y extrañas relaciones de m2 y ambientes, perjudica al modelo que queremos trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5217fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'PH'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean()\n",
    "#al igual que apartment, es un tipo de propiedad estable con sus datos, un poco de inconsistencias en los extremos pero correcto en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'store'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean() \n",
    "#no es tan representativo ya que 'store' no es común que posea ambientes, por lo que ensucia un poco el df en base a lo que queremos analizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726e806a",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fad304",
   "metadata": {},
   "source": [
    "Pudimos encontrar algunos valores poco representativos dentro de los datos, a estos los llamaremos outliers y buscamos eliminarlos para tener una moestra más exacta de los datos.\n",
    "Para eliminarlos trabajaremos con el método no supervisado llamado DBSCAN, el cual crea clusters en función de los parametros eps y min_samples.\n",
    "Esprilon (eps) representa el radio dentro del cual buscamos un cierto número de datos vecinos (min_samples) los cuales forman un cluster. Los valores que no tienen otros valores vecinos dentro del radio estipulado, se consideran outliers y el modelo los agrupa en el cluster numerado como -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3dc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un df para analizar los outliers\n",
    "\n",
    "#aquí podemos setear las variables que queramos analizar, pero hay que modificar un par de códigos más abajo si agregamos la 3ra feature\n",
    "feature_0 = 'place_name'\n",
    "feature_1 = 'surface_covered_in_m2'\n",
    "feature_2 = 'ambientes'\n",
    "feature_3 = 'surface_total_in_m2'\n",
    "\n",
    "features_dbscan = [feature_0,feature_1,feature_2]#,feature_3]\n",
    "\n",
    "df_dbscan = df_properatti[features_dbscan]#[df_properatti.property_type == 'apartment'] podemos filtrar a su vez por tipo de propiedad\n",
    "df_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd597ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos los datos para tener una idea de su distribución\n",
    "plt.scatter (df_dbscan[feature_1],df_dbscan[feature_2], s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos DBSCAN para asociar los valores a los clusters y los outliers quedan separados en el cluster -1\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_db = StandardScaler().fit_transform(df_dbscan[[feature_1,feature_2]])#,feature_3]])\n",
    "dbscan = DBSCAN(eps = 0.5, min_samples = 5)\n",
    "labels = dbscan.fit_predict(X_db)\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156aa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape de outliers\n",
    "df_dbscan[[feature_1,feature_2]][labels==-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e790b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploteamos los primeros 10 clusters\n",
    "plt.scatter (df_dbscan[feature_1][labels==-1], df_dbscan[feature_2][labels==-1], s=10, c='black') #representan los outliers\n",
    "\n",
    "plt.scatter (df_dbscan[feature_1][labels==0], df_dbscan[feature_2][labels==0], s=10, c='r')\n",
    "plt.scatter (df_dbscan[feature_1][labels==1], df_dbscan[feature_2][labels==1], s=10, c='g')\n",
    "plt.scatter (df_dbscan[feature_1][labels==2], df_dbscan[feature_2][labels==2], s=10, c='yellow')\n",
    "plt.scatter (df_dbscan[feature_1][labels==3], df_dbscan[feature_2][labels==3], s=10, c='brown')\n",
    "plt.scatter (df_dbscan[feature_1][labels==4], df_dbscan[feature_2][labels==4], s=10, c='grey')\n",
    "plt.scatter (df_dbscan[feature_1][labels==5], df_dbscan[feature_2][labels==5], s=10, c='pink')\n",
    "plt.scatter (df_dbscan[feature_1][labels==6], df_dbscan[feature_2][labels==6], s=10, c='purple')\n",
    "plt.scatter (df_dbscan[feature_1][labels==7], df_dbscan[feature_2][labels==7], s=10, c='orange')\n",
    "plt.scatter (df_dbscan[feature_1][labels==8], df_dbscan[feature_2][labels==8], s=10, c='lightblue')\n",
    "plt.scatter (df_dbscan[feature_1][labels==9], df_dbscan[feature_2][labels==9], s=10, c='steelblue')\n",
    "plt.scatter (df_dbscan[feature_1][labels==10], df_dbscan[feature_2][labels==10], s=10, c='lime')\n",
    "\n",
    "plt.xlabel(feature_1)\n",
    "plt.ylabel(feature_2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploteamos los outliers para tener una clara idea de su ubicación\n",
    "plt.scatter(df_dbscan[feature_1][labels==-1], df_dbscan[feature_2][labels==-1], s=10 , c='black')\n",
    "plt.xlabel(feature_1)\n",
    "plt.ylabel(feature_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eecd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos df de outliers para dropear del df original\n",
    "df_to_drop = df_dbscan[labels==-1]\n",
    "df_to_drop.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4db2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Añadimos los datos geoespaciales a los outliers para ver si existe alguna relación dentro del mapa\n",
    "geo_properatti_oultiers = df_to_drop.merge(geo_properatti['geometry'], left_index = True, right_index=True, how = 'left')\n",
    "geo_properatti_oultiers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff930079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos el df de outliers en geodf\n",
    "geo_properatti_oultiers = gpd.GeoDataFrame(geo_properatti_oultiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plasmamos los outliers dentro del mapa de Buenos Aires dividido en departamentos\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "departamentos_arg.plot(ax=ax, color='white', edgecolor='black')\n",
    "\n",
    "\n",
    "geo_properatti_oultiers.plot(color='red', ax=ax, zorder=5, markersize = 50, marker = 'x', label = 'outliers')\n",
    "\n",
    "ax.set_title('Ubicación de los outliers', \n",
    "             pad = 20, \n",
    "             fontdict={'fontsize':20, 'color': '#4873ab'})\n",
    "\n",
    "ax.set_xlabel('Longitud')\n",
    "ax.set_ylabel('Latitud')\n",
    "\n",
    "ax.set(xlim=(-59.1, -58.3), ylim=( -34.95, -34.25))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c5069",
   "metadata": {},
   "source": [
    "Como puede verse, no existe relacion entre los outliers y su ubicación en el mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos los outliers encontrados\n",
    "df_properatti = df_properatti.drop(index = df_to_drop.index)\n",
    "df_properatti = df_properatti.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4aa727",
   "metadata": {},
   "source": [
    "<br>\n",
    "Chequeamos si DBSCAN pudo detectar los datos no representativos, vemos que algunos errores desaparecieron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'apartment'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean() \n",
    "#un par de datos extraños en los extremos pero es el tipo de propiedad más completo para trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'house'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean() \n",
    "#muchos datos vacíos y extrañas relaciones de m2 y ambientes, perjudica al modelo qeu qeuremos trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba610c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'PH'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean()\n",
    "#al igual que atartment, es un tipo de propiedad estable con sus datos, un poco de inconsistencias en los extremos pero correcto en general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3965e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti[df_properatti.property_type == 'store'].groupby(['property_type','surface_total_in_m2'])['ambientes'].mean() \n",
    "#no es tan representativo ya que 'store' no es comun que posea ambientes, por lo que ensucia un poco el df en base a lo que queremos analizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201aaa5b",
   "metadata": {},
   "source": [
    "Vemos que, antes de eliminar los registros con NaN en la columna \"ambientes\" teníamos 43348 registros, y luego de eliminarlos tenemos 30576 registros en el Dataframe \"df_properatti\".<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f5905",
   "metadata": {},
   "source": [
    "#### Ahora veamos cómo se relacionan las columnas entre sí con la Matriz de Correlación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01742b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos la matriz de correlacion:\n",
    "fig, ax = plt.subplots(figsize=(9.5,9.5))       \n",
    "sns.heatmap(df_properatti.corr() , square=True, annot=True, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la matriz de Correlación en Seaborn usando a heatmap:\n",
    "sns.heatmap(df_properatti.corr() , vmin=-1, vmax=1, center= 0, cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415cbdf",
   "metadata": {},
   "source": [
    "Del gráfico anterior, observamos que hay una alta correlación entre el Precio Total en USD ('ARS_to_USD_corregido'), la Superficie Total ('surface_total_in_m2'), la Superficie Cubierta ('surface_covered_in_m2') y los Ambientes (ambientes), de modo que vamos a usar dichas features por separado para intentar predecir el Precio Total en USD con el modelo de Regresión Lineal Simple. Posteriormente combinaremos todas las features en una Regresión Lineal Multiple para ver si tenemos una mayor precicion a la hora de predecir el valor de una propiedad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c3098",
   "metadata": {},
   "source": [
    "En este gráfico podemos ver la relacion lineal entre todas las variables del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76473694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aquí podemos ver la relación 1 a 1 entre todas las variables del df\n",
    "sns.pairplot(df_properatti);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a renombrar la columna llamada 'place_name' a 'barrio' para mayor comodidad:\n",
    "df_properatti.rename(columns={'place_name':'barrio'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c915b2",
   "metadata": {},
   "source": [
    "<a id=\"section_modelado\"></a> \n",
    "## 3. Feature Engineering (Modelado)\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0becb30",
   "metadata": {},
   "source": [
    "Para comenzar el modelado, transformaremos las variables Categóricas en dummies, luego más adelante normalizaremos las variables para facilitar la corrida de los distintos modelos (Cuando veamos la Regresión Lineal Múltiple).\n",
    "\n",
    "Vamos a crear variables Dummies para las siguientes variables Categóricas:\n",
    "- property_type\n",
    "- CABA_or_GBA\n",
    "- barrio (anteriormente llamada 'place_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos variables dummies de las variables categóricas:\n",
    "\n",
    "property_type_dummies=pd.get_dummies(df_properatti.property_type, prefix= 'property_type', drop_first=True)\n",
    "\n",
    "CABA_or_GBA_dummies=pd.get_dummies(df_properatti.CABA_or_GBA, drop_first=True) #quité el pefix porque era medio confuso CABA_or_GBA_GBA\n",
    "\n",
    "barrio_dummies=pd.get_dummies(df_properatti.barrio, drop_first=True) #quite el prefix porque necesito los nombres de los barrios limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b705d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenamos el Dataframe original y los Dummy Dataframes (axis=0 significa filas, axis=1 significa columnas):\n",
    "df_properatti=pd.concat([df_properatti,property_type_dummies], axis=1)\n",
    "df_properatti=pd.concat([df_properatti,CABA_or_GBA_dummies], axis=1)\n",
    "df_properatti=pd.concat([df_properatti,barrio_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8537f3d",
   "metadata": {},
   "source": [
    "### Chequeamos si se generaron correctamente las columnas de las variables Dummies en nuestro Dataframe original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40833e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properatti.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de11e2b",
   "metadata": {},
   "source": [
    "<a id=\"section_lineal_simple\"></a> \n",
    "## 4. Regresión Lineal Simple\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2db741",
   "metadata": {},
   "source": [
    "#### Importamos todas las librerías necesarias para la Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f557c0",
   "metadata": {},
   "source": [
    "Representamos gráficamente las relaciones lineales entre las features elegidas por su correlación con el precio total en USD (surface_total_in_m2, ambientes y surface_covered_in_m2), y observamos como son las pendientes según son de CABA o GBA, o según el tipo de propiedad y como útltima imagen vemos la regresión lineal en general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e83ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el Scatterplot entre 'surface_total_in_m2' y 'ARS_to_USD' para ver a priori qué correlación hay:\n",
    "sns.pairplot(df_properatti, x_vars=[\"surface_total_in_m2\", \"ambientes\", \"surface_covered_in_m2\"], y_vars=[\"ARS_to_USD_corregido\"],\n",
    "              hue='CABA_or_GBA', height=5, aspect=.8, kind=\"reg\").set(title = 'Distribucion por CABA o GBA');\n",
    "sns.pairplot(df_properatti, x_vars=[\"surface_total_in_m2\", \"ambientes\", \"surface_covered_in_m2\"], y_vars=[\"ARS_to_USD_corregido\"],\n",
    "              hue='property_type', height=5, aspect=.8, kind=\"reg\").set(title = 'Distribucion por tipo de propiedad');\n",
    "sns.pairplot(df_properatti, x_vars=[\"surface_total_in_m2\", \"ambientes\", \"surface_covered_in_m2\"], y_vars=[\"ARS_to_USD_corregido\"],\n",
    "              height=5, aspect=.8, kind=\"reg\").set(title = 'Distribucion en general');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b957d3",
   "metadata": {},
   "source": [
    "Definimos una función y la llamamos 'train_test_error' que requiera como parámetro una lista de features (feature_cols), genere la matriz de variables independientes 'X' y el array de la variable Target 'y' para luego hacer el split entre train y test reservando un 25% de las observaciones para testeo, y que finalmente imprima los errores MAE, MSE, RMSE y R2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_error(feature_cols):\n",
    "    \n",
    "    #Armamos nuestran matriz de features (X) y nuestro vector objetivo (y):\n",
    "    X = df_properatti[feature_cols]\n",
    "    y = df_properatti.ARS_to_USD_corregido\n",
    "    \n",
    "    #Separamos los datos en sets de testeo y training:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "    #Instanciamos el modelo:\n",
    "    linreg = linear_model.LinearRegression()\n",
    "    \n",
    "    #Entrenamos el modelo:\n",
    "    reg_lineal = linreg.fit(X_train, y_train)\n",
    "    \n",
    "    #Calculamos el y_pred:\n",
    "    y_pred = linreg.predict(X_test)\n",
    "    \n",
    "    #Guardamos en una variable al Score (R2) del modelo para poder comparar:\n",
    "    R2_reg_lineal= reg_lineal.score(X_test, y_test)\n",
    "    \n",
    "    print (feature_cols)\n",
    "    #Imprimimos los coeficientes:\n",
    "    print('Intercept: ', linreg.intercept_.round(3))\n",
    "    print('Coeficients: ',linreg.coef_.round(3))\n",
    "    #para observarlo mejor miramos el nombre de la variable con el coeficiente:\n",
    "    print(list(zip(feature_cols, linreg.coef_.round(3))))\n",
    "    \n",
    "    #Imprimimos todas las métricas que nos interesan para poder comparar:\n",
    "    print ('y_test_sample:' , (y_test.values[0:5]))\n",
    "    print ('y_pred_sample:', y_pred[0:5].astype(int))\n",
    "    print ('MAE: ', metrics.mean_absolute_error(y_test, y_pred).round(3))\n",
    "    print ('MSE: ', metrics.mean_squared_error(y_test, y_pred).round(3))\n",
    "    print ('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\n",
    "    print ('R2_train: ', reg_lineal.score(X_train, y_train).round(3))\n",
    "    print ('R2_test: ', reg_lineal.score(X_test, y_test).round(3))\n",
    "    print(f'\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8a7e5",
   "metadata": {},
   "source": [
    "Utilizando la función 'train_test_error' recién creada, probamos y comparamos diferentes ensambles de features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_error(['surface_total_in_m2'])\n",
    "train_test_error(['surface_covered_in_m2'])\n",
    "train_test_error(['ambientes'])\n",
    "train_test_error(['estrenar'])\n",
    "train_test_error(['cochera'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7965cbe",
   "metadata": {},
   "source": [
    "Como la función no almacena las variables creadas, procedemos a trabajar todas las variables (features) por separado para poder almacenar sus resultados y compararlos al final del trabajo y así ver cuál modelo resulto ser el más representaivo.\n",
    "Agregamos al final de cada ejecución un gráfico que representa los valores predichos con el modelo vs los valores reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb59fab",
   "metadata": {},
   "source": [
    "<a id=\"section_4.1\"></a> \n",
    "### 4.1 Regresión Lineal Simple - _Superficie Total_\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos nuestran matriz de features (X) y nuestro vector objetivo (y):\n",
    "feature_cols = ['surface_total_in_m2']\n",
    "X = df_properatti[feature_cols]\n",
    "y = df_properatti.ARS_to_USD_corregido\n",
    "    \n",
    "#Separamos los datos en sets de testeo y training:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "#Instanciamos el modelo:\n",
    "linreg = linear_model.LinearRegression()\n",
    "    \n",
    "#Entrenamos el modelo:\n",
    "reg_lineal_m2 = linreg.fit(X_train, y_train)\n",
    "    \n",
    "#Calculamos el y_pred:\n",
    "y_pred = linreg.predict(X_test)\n",
    "    \n",
    "#Guardamos en una variable al Score (R2) del modelo para poder comparar:\n",
    "R2_reg_lineal_m2= reg_lineal_m2.score(X_test, y_test)\n",
    "\n",
    "print (feature_cols)    \n",
    "#Imprimimos los coeficientes:\n",
    "print('Intercept: ', linreg.intercept_.round(3))\n",
    "print('Coeficients: ',linreg.coef_.round(3))\n",
    "#para observarlo mejor miramos el nombre de la variable con el coeficiente:\n",
    "print(list(zip(feature_cols, linreg.coef_.round(3))))\n",
    "    \n",
    "#Imprimimos todas las métricas que nos interesan para poder comparar:\n",
    "print ('y_test_sample:' , y_test.values[0:10])\n",
    "print ('y_pred_sample:', y_pred[0:10].astype(int))\n",
    "print ('MAE: ', metrics.mean_absolute_error(y_test, y_pred).round(3))\n",
    "print ('MSE: ', metrics.mean_squared_error(y_test, y_pred).round(3))\n",
    "print ('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\n",
    "print ('R2_train: ', reg_lineal_m2.score(X_train, y_train).round(3))\n",
    "print ('R2_test: ', reg_lineal_m2.score(X_test, y_test).round(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lineal_simple_model_m2 = {'Modelo': \"Regresion Lineal Simple M2\",\n",
    "                        'Alcance del modelo': 'Buenos Aires',\n",
    "                        'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                        'Cantidad de Observaciones': len(df_properatti),\n",
    "                        'R2_train': reg_lineal_m2.score(X_train, y_train),\n",
    "                        'R2_test': R2_reg_lineal_m2,\n",
    "                        'Intercepto': reg_lineal_m2.intercept_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,y, '-.',c='grey')\n",
    "plt.scatter(y_pred, y_test, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicciones (y_pred)\")\n",
    "plt.ylabel(\"Valores reales (y_test)\")\n",
    "plt.title(feature_cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011f8ba",
   "metadata": {},
   "source": [
    "<a id=\"section_4.2\"></a> \n",
    "### 4.2 Regresión Lineal Simple - _Ambientes_\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cf5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos nuestran matriz de features (X) y nuestro vector objetivo (y):\n",
    "feature_cols = ['ambientes']\n",
    "X = df_properatti[feature_cols]\n",
    "y = df_properatti.ARS_to_USD_corregido\n",
    "    \n",
    "#Separamos los datos en sets de testeo y training:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "#Instanciamos el modelo:\n",
    "linreg = linear_model.LinearRegression()\n",
    "    \n",
    "#Entrenamos el modelo:\n",
    "reg_lineal_amb = linreg.fit(X_train, y_train)\n",
    "    \n",
    "#Calculamos el y_pred:\n",
    "y_pred = linreg.predict(X_test)\n",
    "    \n",
    "#Guardamos en una variable al Score (R2) del modelo para poder comparar:\n",
    "R2_reg_lineal_amb= reg_lineal_amb.score(X_test, y_test)\n",
    "\n",
    "print (feature_cols)\n",
    "#Imprimimos los coeficientes:\n",
    "print('Intercept: ', linreg.intercept_.round(3))\n",
    "print('Coeficients: ',linreg.coef_.round(3))\n",
    "#para observarlo mejor miramos el nombre de la variable con el coeficiente:\n",
    "print(list(zip(feature_cols, linreg.coef_.round(3))))\n",
    "    \n",
    "#Imprimimos todas las métricas que nos interesan para poder comparar:\n",
    "print ('y_test_sample:' , y_test.values[0:10])\n",
    "print ('y_pred_sample:', y_pred[0:10].astype(int))\n",
    "print ('MAE: ', metrics.mean_absolute_error(y_test, y_pred).round(3))\n",
    "print ('MSE: ', metrics.mean_squared_error(y_test, y_pred).round(3))\n",
    "print ('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\n",
    "print ('R2_train: ', reg_lineal_amb.score(X_train, y_train).round(3))\n",
    "print ('R2_test: ', reg_lineal_amb.score(X_test, y_test).round(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afeb013",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lineal_simple_model_amb = {'Modelo': \"Regresion Lineal Simple Ambientes\",\n",
    "                        'Alcance del modelo': 'Buenos Aires',\n",
    "                        'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                        'Cantidad de Observaciones': len(df_properatti),\n",
    "                        'R2_train': reg_lineal_amb.score(X_train, y_train),\n",
    "                        'R2_test': R2_reg_lineal_amb,\n",
    "                        'Intercepto': reg_lineal_amb.intercept_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af535a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos el modelo\n",
    "plt.scatter(y_pred, y_test, alpha = 0.5,s=30, c='r', marker='x', zorder=10)\n",
    "plt.plot(y,y, '-.',c='blue')  # con esto graficamos la recta y=x , o sea que ambas variables x e y tomen el mismo valor\n",
    "plt.xlabel(\"Predicciones (y_pred)\")\n",
    "plt.ylabel(\"Valores reales (y_test)\")\n",
    "plt.title(feature_cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29e165",
   "metadata": {},
   "source": [
    "<a id=\"section_4.3\"></a> \n",
    "### 4.3 Regresión Lineal Simple - _Estrenar_\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a888e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos nuestran matriz de features (X) y nuestro vector objetivo (y):\n",
    "feature_cols = ['estrenar']\n",
    "X = df_properatti[feature_cols]\n",
    "y = df_properatti.ARS_to_USD_corregido\n",
    "    \n",
    "#Separamos los datos en sets de testeo y training:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "#Instanciamos el modelo:\n",
    "linreg = linear_model.LinearRegression()\n",
    "    \n",
    "#Entrenamos el modelo:\n",
    "reg_lineal_est = linreg.fit(X_train, y_train)\n",
    "    \n",
    "#Calculamos el y_pred:\n",
    "y_pred = linreg.predict(X_test)\n",
    "    \n",
    "#Guardamos en una variable al Score (R2) del modelo para poder comparar:\n",
    "R2_reg_lineal_est= reg_lineal_est.score(X_test, y_test)\n",
    "\n",
    "print(feature_cols)\n",
    "#Imprimimos los coeficientes:\n",
    "print('Intercept: ', linreg.intercept_.round(3))\n",
    "print('Coeficients: ',linreg.coef_.round(3))\n",
    "#para observarlo mejor miramos el nombre de la variable con el coeficiente:\n",
    "print(list(zip(feature_cols, linreg.coef_.round(3))))\n",
    "    \n",
    "#Imprimimos todas las métricas que nos interesan para poder comparar:\n",
    "print ('y_test_sample:' , y_test.values[0:10])\n",
    "print ('y_pred_sample:', y_pred[0:10].astype(int))\n",
    "print ('MAE: ', metrics.mean_absolute_error(y_test, y_pred).round(3))\n",
    "print ('MSE: ', metrics.mean_squared_error(y_test, y_pred).round(3))\n",
    "print ('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\n",
    "print ('R2_train: ', reg_lineal_est.score(X_train, y_train).round(3))\n",
    "print ('R2_test: ', reg_lineal_est.score(X_test, y_test).round(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da051f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lineal_simple_model_est = {'Modelo': \"Regresion Lineal Simple Estrenar\",\n",
    "                        'Alcance del modelo': 'Buenos Aires',\n",
    "                        'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                        'Cantidad de Observaciones': len(df_properatti),\n",
    "                        'R2_train': reg_lineal_est.score(X_train, y_train),\n",
    "                        'R2_test': R2_reg_lineal_est,\n",
    "                        'Intercepto': reg_lineal_est.intercept_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ba92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos el modelo\n",
    "plt.scatter(y_pred, y_test, alpha = 0.5,s=30, c='r', marker='x', zorder=10)\n",
    "plt.plot(y,y, '-.',c='blue')  # con esto graficamos la recta y=x , o sea que ambas variables x e y tomen el mismo valor\n",
    "plt.xlabel(\"Predicciones (y_pred)\")\n",
    "plt.ylabel(\"Valores reales (y_test)\")\n",
    "plt.title(feature_cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6d924",
   "metadata": {},
   "source": [
    "<a id=\"section_4.4\"></a> \n",
    "### 4.4 Regresión Lineal Simple - _Cochera_\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos nuestran matriz de features (X) y nuestro vector objetivo (y):\n",
    "feature_cols = ['cochera']\n",
    "X = df_properatti[feature_cols]\n",
    "y = df_properatti.ARS_to_USD_corregido\n",
    "    \n",
    "#Separamos los datos en sets de testeo y training:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "#Instanciamos el modelo:\n",
    "linreg = linear_model.LinearRegression()\n",
    "    \n",
    "#Entrenamos el modelo:\n",
    "reg_lineal_coche = linreg.fit(X_train, y_train)\n",
    "    \n",
    "#Calculamos el y_pred:\n",
    "y_pred = linreg.predict(X_test)\n",
    "    \n",
    "#Guardamos en una variable al Score (R2) del modelo para poder comparar:\n",
    "R2_reg_lineal_coche= reg_lineal_coche.score(X_test, y_test)\n",
    "    \n",
    "print (feature_cols)\n",
    "#Imprimimos los coeficientes:\n",
    "print('Intercept: ', linreg.intercept_.round(3))\n",
    "print('Coeficients: ',linreg.coef_.round(3))\n",
    "#para observarlo mejor miramos el nombre de la variable con el coeficiente:\n",
    "print(list(zip(feature_cols, linreg.coef_.round(3))))\n",
    "    \n",
    "#Imprimimos todas las métricas que nos interesan para poder comparar:\n",
    "print ('y_test_sample:' , y_test.values[0:10])\n",
    "print ('y_pred_sample:', y_pred[0:10].astype(int))\n",
    "print ('MAE: ', metrics.mean_absolute_error(y_test, y_pred).round(3))\n",
    "print ('MSE: ', metrics.mean_squared_error(y_test, y_pred).round(3))\n",
    "print ('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\n",
    "print ('R2_train: ', reg_lineal_coche.score(X_train, y_train).round(3))\n",
    "print ('R2_test: ', reg_lineal_coche.score(X_test, y_test).round(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lineal_simple_model_cochera = {'Modelo': \"Regresion Lineal Simple Cochera\",\n",
    "                        'Alcance del modelo': 'Buenos Aires',\n",
    "                        'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                        'Cantidad de Observaciones': len(df_properatti),\n",
    "                        'R2_train': reg_lineal_coche.score(X_train, y_train),\n",
    "                        'R2_test': R2_reg_lineal_coche,\n",
    "                        'Intercepto': reg_lineal_coche.intercept_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos el modelo\n",
    "plt.scatter(y_pred, y_test, alpha = 0.5,s=30, c='r', marker='x', zorder=10)\n",
    "plt.plot(y,y, '-.',c='blue')  # con esto graficamos la recta y=x , o sea que ambas variables x e y tomen el mismo valor\n",
    "plt.xlabel(\"Predicciones (y_pred)\")\n",
    "plt.ylabel(\"Valores reales (y_test)\")\n",
    "plt.title(feature_cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193226a",
   "metadata": {},
   "source": [
    "<a id=\"section_4.5\"></a> \n",
    "### 4.5 Regresión Lineal Simple - _Superficie Cubierta_\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos nuestran matriz de features (X) y nuestro vector objetivo (y):\n",
    "feature_cols = ['surface_covered_in_m2']\n",
    "X = df_properatti[feature_cols]\n",
    "y = df_properatti.ARS_to_USD_corregido\n",
    "    \n",
    "#Separamos los datos en sets de testeo y training:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "#Instanciamos el modelo:\n",
    "linreg = linear_model.LinearRegression()\n",
    "    \n",
    "#Entrenamos el modelo:\n",
    "reg_lineal_scm2 = linreg.fit(X_train, y_train)\n",
    "    \n",
    "#Calculamos el y_pred:\n",
    "y_pred = linreg.predict(X_test)\n",
    "    \n",
    "#Guardamos en una variable al Score (R2) del modelo para poder comparar:\n",
    "R2_reg_lineal_scm2= reg_lineal_scm2.score(X_test, y_test)\n",
    "    \n",
    "print (feature_cols)\n",
    "#Imprimimos los coeficientes:\n",
    "print('Intercept: ', linreg.intercept_.round(3))\n",
    "print('Coeficients: ',linreg.coef_.round(3))\n",
    "#para observarlo mejor miramos el nombre de la variable con el coeficiente:\n",
    "print(list(zip(feature_cols, linreg.coef_.round(3))))\n",
    "    \n",
    "#Imprimimos todas las métricas que nos interesan para poder comparar:\n",
    "print ('y_test_sample:' , y_test.values[0:10])\n",
    "print ('y_pred_sample:', y_pred[0:10].astype(int))\n",
    "print ('MAE: ', metrics.mean_absolute_error(y_test, y_pred).round(3))\n",
    "print ('MSE: ', metrics.mean_squared_error(y_test, y_pred).round(3))\n",
    "print ('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\n",
    "print ('R2_train: ', reg_lineal_scm2.score(X_train, y_train).round(3))\n",
    "print ('R2_test: ', reg_lineal_scm2.score(X_test, y_test).round(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lineal_simple_model_scm2 = {'Modelo': \"Regresion Lineal Simple m2 cubiertos\",\n",
    "                        'Alcance del modelo': 'Buenos Aires',\n",
    "                        'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                        'Cantidad de Observaciones': len(df_properatti),\n",
    "                        'R2_train': reg_lineal_scm2.score(X_train, y_train),\n",
    "                        'R2_test': R2_reg_lineal_scm2,\n",
    "                        'Intercepto': reg_lineal_scm2.intercept_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c393e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos el modelo\n",
    "plt.scatter(y_pred, y_test, alpha = 0.5,s=30, c='r', marker='x', zorder=10)\n",
    "plt.plot(y,y, '-.',c='blue')  # con esto graficamos la recta y=x , o sea que ambas variables x e y tomen el mismo valor\n",
    "plt.xlabel(\"Predicciones (y_pred)\")\n",
    "plt.ylabel(\"Valores reales (y_test)\")\n",
    "plt.title(feature_cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5852e",
   "metadata": {},
   "source": [
    "#### Conclusión de la Regresión Lineal Simple aplicada a las features de manera individual\n",
    "Vemos que es una dispersión que no se puede representar con una recta de una manera que tenga precisión, ya que cualquier recta que se trace, por más de  que minimice los errores, siempre va a haber muchos puntos por encima de la recta y muchos por debajo. Por lo que ya sabemos de antemano que este modelo de Regresión no va a lograr un nivel aceptable de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6725ab",
   "metadata": {},
   "source": [
    "<a id=\"section_lineal_multiple\"></a> \n",
    "## 5. Regresión Lineal Multiple\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be4f15",
   "metadata": {},
   "source": [
    "Podemos observar que el modelo de Regresión Lineal Simple es demasiado simple para poder obtener un insight lo suficientemente convincente, por lo cual vamos a aplicar un modelo con todas las features al mismo tiempo (Regrsión Lineal Multiple) y posteriormente otros dos modelos que incluyen técnicas de Regularizacion (Ridge y Lasso) para mejorar el R2 del modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c2e17",
   "metadata": {},
   "source": [
    "<a id=\"section_5.1\"></a> \n",
    "## 5.1 Regresión Lineal Multiple _SIN Regularización_ \n",
    "(sin estandarizacion de variables numericas ni variables dummies para variables categoricas)\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0a218",
   "metadata": {},
   "source": [
    "Empecemos realizando una Regresión Lineal Múltiple sin Estandarización de Features Numéricas ni Creación de variables Dummies, para ver cómo nos da el Modelo. \n",
    "Usaremos 5 variables predictoras en nuestro modelo de Regresión Lineal Múltiple para empezar: \"surface_total_in_m2\", \"ambientes\", \"estrenar\", \"cochera\" y \"surface_covered_in_m2\". \n",
    "Crearemos la serie \"feature_cols\" con esas variables, y luego la meteremos en la función \"train_test_error\" que habíamos creado para que nos evalúe el Modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546934f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_error(['surface_total_in_m2','surface_covered_in_m2','ambientes','estrenar','cochera']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armamos nuestran matriz de features (X) y nuestro vector objetivo (y):\n",
    "feature_cols = ['surface_total_in_m2','surface_covered_in_m2','ambientes','estrenar','cochera']\n",
    "X = df_properatti[feature_cols]\n",
    "y = df_properatti.ARS_to_USD_corregido\n",
    "    \n",
    "#Separamos los datos en sets de testeo y training:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "#Instanciamos el modelo:\n",
    "linreg = linear_model.LinearRegression()\n",
    "    \n",
    "#Entrenamos el modelo:\n",
    "reg_lineal_mult = linreg.fit(X_train, y_train)\n",
    "    \n",
    "#Calculamos el y_pred:\n",
    "y_pred = linreg.predict(X_test)\n",
    "    \n",
    "#Guardamos en una variable al Score (R2) del modelo para poder comparar:\n",
    "R2_reg_lineal_mult = reg_lineal_mult.score(X_test, y_test)\n",
    "    \n",
    "print (feature_cols)\n",
    "#Imprimimos los coeficientes:\n",
    "print('Intercept: ', linreg.intercept_.round(3))\n",
    "print('Coeficients: ',linreg.coef_.round(3))\n",
    "#para observarlo mejor miramos el nombre de la variable con el coeficiente:\n",
    "print(list(zip(feature_cols, linreg.coef_.round(3))))\n",
    "    \n",
    "#Imprimimos todas las métricas que nos interesan para poder comparar:\n",
    "print ('y_test_sample:' , y_test.values[0:10])\n",
    "print ('y_pred_sample:', y_pred[0:10].astype(int))\n",
    "print ('MAE: ', metrics.mean_absolute_error(y_test, y_pred).round(3))\n",
    "print ('MSE: ', metrics.mean_squared_error(y_test, y_pred).round(3))\n",
    "print ('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\n",
    "print ('R2_train: ', reg_lineal_mult.score(X_train, y_train).round(3))\n",
    "print ('R2_test: ', reg_lineal_mult.score(X_test, y_test).round(3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario con la informacion para luego amar un dataframe y comparar los modelos:\n",
    "\n",
    "reg_multiple_sin_reg_model = {'Modelo': \"Regresion Lineal Múltiple sin Regularización\",\n",
    "                        'Alcance del modelo': 'Buenos Aires',\n",
    "                        'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                        'Cantidad de Observaciones': len(df_properatti),\n",
    "                        'R2_train': reg_lineal_mult.score(X_train, y_train),\n",
    "                        'R2_test': reg_lineal_mult.score(X_test, y_test),\n",
    "                        'Intercepto': reg_lineal_mult.intercept_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41510273",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y,y, '-.',c='grey')\n",
    "plt.scatter(y_pred, y_test, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"ARS_to_USD_predict\")\n",
    "plt.ylabel(\"ARS_to_USD_real\")\n",
    "plt.title('Comparación entre y_pred y los valores y reales', pad = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3a884",
   "metadata": {},
   "source": [
    "#### Conclusión de la Regresión Lineal Multiple aplicada a las features de manera grupal\n",
    "Ahora podemos ver como la tendencia de las predicciones se acercan más a la linea diagonal (que es la recta a la cual tiene que tender las predicciones) por lo cual podemos decir que el modelo mejoró con respecto a las regresiones lineales simples, pero aún se puede ajustar para una predicción más precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_properatti.surface_total_in_m2, y, s=30, c='purple', marker='+', zorder=10, alpha = 0.5, label = \"m2 total\")\n",
    "plt.scatter(df_properatti.surface_covered_in_m2, y, s=30, c='black', marker='.', zorder=10, alpha = 0.5, label = \"m2 cubierto\")\n",
    "plt.scatter(df_properatti.ambientes, y, s=10, c='green', marker='o', zorder=10, alpha = 0.5, label = \"ambientes\")\n",
    "plt.xlabel(\"Valores estandarizados\")\n",
    "plt.ylabel(\"Precio en USD\")\n",
    "plt.title('Valores estandarizados')\n",
    "plt.legend(loc=\"center\", bbox_to_anchor=(0.5, -.20), shadow=False, ncol=2)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(y,y, '-.',c='grey')\n",
    "plt.scatter(y_pred, y_test, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Precio en USD predichos\")\n",
    "plt.ylabel(\"Precio en USD\")\n",
    "plt.title('y_real vs y_pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b4381",
   "metadata": {},
   "source": [
    "<a id=\"section_5.2\"></a> \n",
    "## 5.2 Regresión Lineal Multiple _CON Regularización_ \n",
    "(con estandarizacion de variables numericas ni variables dummies para variables categoricas)\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce7b03",
   "metadata": {},
   "source": [
    "### Estandarizacion de features numericas:\n",
    "Primero vamos a Normalizar (Estandarizar) las Features Numéricas: Con esto llevamos a las variables numéricas a la misma escala (desv estandar = 1 y media = 0) y le damos el mismo peso a todas. \n",
    "Las variables dummies no es necesario que sean normalizadas ya que son variables dicotomicas (0 o 1). \n",
    "Recordar que en la lista numericals vamos a poner a todas las variables numéricas, pero no a la variable Target (precio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías de Regularización:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4267e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregaba 2 varables cuadráticas para ver si el modelo mejoraba, pero no lo hizo..\n",
    "# df_properatti['ambientes_2'] = df_properatti.ambientes * df_properatti.ambientes\n",
    "# df_properatti['sup_total_2'] = df_properatti.surface_total_in_m2 * df_properatti.surface_total_in_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usamos StandarScaler como para escaloar los valores ya que MaxMin puede causar problemas con los outliers que puedan haber\n",
    "numericals = ['surface_total_in_m2','surface_covered_in_m2', 'ambientes']#,'sup_total_2', 'ambientes_2']\n",
    "\n",
    "X = df_properatti[numericals]\n",
    "scaler = StandardScaler() \n",
    "X_std = scaler.fit_transform(X)    \n",
    "std_df = pd.DataFrame(X_std)\n",
    "std_df.columns = [i + '_std' for i in numericals] \n",
    "std_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos que esté normalizado el std_df:\n",
    "std_df.describe().round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c185a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploteamos los valores estandarizados vs los no estandarizados\n",
    "plt.scatter(df_properatti.surface_total_in_m2, y, s=30, c='purple', marker='+', zorder=10, alpha = 0.5, label = \"m2 total\")\n",
    "plt.scatter(df_properatti.surface_covered_in_m2, y, s=30, c='black', marker='.', zorder=10, alpha = 0.5, label = \"m2 cubierto\")\n",
    "plt.scatter(df_properatti.ambientes, y, s=10, c='green', marker='o', zorder=10, alpha = 0.5, label = \"ambientes\")\n",
    "plt.xlabel(\"Valores NO estandarizados\")\n",
    "plt.ylabel(\"Precio en USD\")\n",
    "plt.title('Valores NO estandarizados')\n",
    "plt.legend(loc=\"center\", bbox_to_anchor=(1.2, .50), shadow=False)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(std_df.surface_total_in_m2_std, y, s=30, c='purple', marker='+', zorder=10, alpha = 0.5, label = \"m2 total\")\n",
    "plt.scatter(std_df.surface_covered_in_m2_std, y, s=30, c='black', marker='.', zorder=10, alpha = 0.5, label = \"m2 cubierto\")\n",
    "plt.scatter(std_df.ambientes_std, y, s=10, c='green', marker='o', zorder=10, alpha = 0.5, label = \"ambientes\")\n",
    "plt.xlabel(\"Valores estandarizados\")\n",
    "plt.ylabel(\"Precio en USD\")\n",
    "plt.title('Valores estandarizados')\n",
    "plt.legend(loc=\"center\", bbox_to_anchor=(1.2, .50), shadow=False)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(y,y, '-.',c='grey')\n",
    "plt.scatter(y_pred, y_test, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Precio en USD predichos\")\n",
    "plt.ylabel(\"Precio en USD\")\n",
    "plt.title('y_real vs y_pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3e7df",
   "metadata": {},
   "source": [
    "Podemos ver como tiene efecto la estandarización de los datos y podemos tener una mejor visión de los datos dentro de un mismo gráfico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2ab7f",
   "metadata": {},
   "source": [
    "<br>Vamos a crear el Dummy Dataframe, en el que vamos a seleccionar solamente a las columnas dummies que habíamos creado antes en el Dataframe \"df_properatti\". Recordemos que las variables dummies las habíamos concatenado a nuestro dataframe original, y que empezaban en la columna 13 del Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a72cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_df = df_properatti.iloc[:,13:]#262] si es que uso las variables cuadráticas creadas más arriba\n",
    "dummies_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f03d67",
   "metadata": {},
   "source": [
    "Entonces X, que es el dataset de features que usamos para el entrenamiento, va a ser concatenar todas las variables dummies que construimos antes (dummies_df) con todas las variables numéricas estandarizadas (std_df) más las variables Estrenar y Cochera. \n",
    "Este X es básicamente nuestro nuevo dataset que vamos a usar para entrenar el modelo de Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84773414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores que usaremos de ahora en adelante como X e y\n",
    "X = pd.concat([dummies_df, std_df,df_properatti[['estrenar', 'cochera']].astype(int)], axis=1)\n",
    "y = df_properatti.ARS_to_USD_corregido\n",
    "\n",
    "X.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3aba0",
   "metadata": {},
   "source": [
    "## Stats model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2236d9",
   "metadata": {},
   "source": [
    "Trabajamos sobre con Stats Model para conocer una descripción profunda de los coeficientes con los que estamos trabajando en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f69c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_const = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "mod_summary = model.summary()\n",
    "mod_param = model.params\n",
    "mod_pvalues = np.round(model.pvalues,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3dcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc3e89",
   "metadata": {},
   "source": [
    "<a id=\"section_lineal_ridge\"></a> \n",
    "## 6. Regresión Lineal Ridge\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d91add",
   "metadata": {},
   "source": [
    "Ahora trabajaremos en las regularizaciones, para ello seteamos ciertos parametros que se aplicarán por igual a todos los modelos que usaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fbd77",
   "metadata": {},
   "source": [
    "Creamos la primera regularización, usamos la de Ridge. \n",
    "Guardamos todos los resultados obtenidos para poder compararlos al final con todos los modelos aplicados y poder definir el modelo que más se adapte a nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a71af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos el conjunto en train y test:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#seteamos los valores que usaremos para los cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2206af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciamos el modelo de Ridge:\n",
    "\n",
    "lm_ridgeCV = linear_model.RidgeCV(alphas=[0.0001, 0.0005, 0.001, 0.005, 0.01,\\\n",
    "                                        0.05, 0.1, 1, 5, 10, 100], cv=kf) \n",
    "#Entrenamos el modelo\n",
    "reg_lineal_ridgeCV = lm_ridgeCV.fit(X_train, y_train)\n",
    "\n",
    "#Guardamos el score en una variable:\n",
    "score_ridgeCV = reg_lineal_ridgeCV.score(X_train, y_train)\n",
    "\n",
    "#Prediccion con el set de testeo:\n",
    "print('Score del modelo Ridge:', score_ridgeCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c010a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cuál es el alpha óptimo de Ridge:\n",
    "best_alpha_ridgeCV = lm_ridgeCV.alpha_\n",
    "best_alpha_ridgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = linear_model.Ridge(alpha = best_alpha_ridgeCV, fit_intercept = True, normalize = False)\n",
    "model_fit_ridge = model_ridge.fit(X_train, y_train)\n",
    "print('intercepto_rd :', model_fit_ridge.intercept_.round(3))\n",
    "print('coeficientes_rd :', model_fit_ridge.coef_.round(3))\n",
    "print('score_rd :', model_fit_ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_fit_ridge.coef_ == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de01c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario para luego mostrar los datos en un dataframe:\n",
    "ridge_model = {'Modelo': \"Regresión Lineal Ridge\",\n",
    "                          'Alcance del modelo': 'Buenos Aires',\n",
    "                          'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                          'Cantidad de Observaciones': len(df_properatti),\n",
    "                          'R2_train': score_ridgeCV,\n",
    "                          'R2_test': model_fit_ridge.score(X_test, y_test),\n",
    "                          'Intercepto': reg_lineal_ridgeCV.intercept_,\n",
    "                          'Alpha del modelo': best_alpha_ridgeCV}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a53b42",
   "metadata": {},
   "source": [
    "### Analizamos como reacciona R² y alpha a los cambios en el % de test_size y en el n de folds, en la regularización de Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3e391",
   "metadata": {},
   "source": [
    "Queremos conocer los movimientos de R2 tanto de train como de test cuando alteramos los valores del porcentaje del test set sobre el train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizamos los cambios de R2 y alpha cuando variamos el % de test usado para testear el modelo\n",
    "\n",
    "cv_check_test_size = []\n",
    "for i in np.linspace (0.05, 0.95, num=19):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=42)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "    lm_ridgeCV = linear_model.RidgeCV(alphas=[0.0001, 0.0005, 0.001, 0.005, 0.01,\\\n",
    "                                        0.05, 0.1, 1, 1.5, 2, 2.5, 3, 4, 5, 10, 100], cv=kf) \n",
    "    reg_lineal_ridgeCV = lm_ridgeCV.fit(X_train, y_train)\n",
    "    score_ridgeCV_train = reg_lineal_ridgeCV.score(X_train, y_train)\n",
    "    score_ridgeCV_test = reg_lineal_ridgeCV.score(X_test, y_test)\n",
    "    best_alpha_ridgeCV = lm_ridgeCV.alpha_\n",
    "\n",
    "    cv_check_test_size_1 = (i.round(2), score_ridgeCV_train, score_ridgeCV_test,best_alpha_ridgeCV)\n",
    "    cv_check_test_size.append(cv_check_test_size_1)\n",
    "\n",
    "df_array_test_size = pd. DataFrame(cv_check_test_size, columns = ['test_size','R2_train','R2_test','best_alpha'])\n",
    "\n",
    "df_array_test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficos de los R2\n",
    "plt.plot(df_array_test_size.test_size, df_array_test_size.R2_train, c='r', label='R2_train'),\n",
    "plt.plot(df_array_test_size.test_size, df_array_test_size.R2_test, c='g', label='R2_test'),\n",
    "plt.xlabel('Test Size')\n",
    "plt.ylabel('R2 values')\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()\n",
    "\n",
    "#grafico de alpha\n",
    "plt.plot(df_array_test_size.test_size, df_array_test_size.best_alpha, c='b', label = 'alpha'),\n",
    "plt.xlabel('Test Size'),\n",
    "plt.ylabel('Alpha values'),\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5c2e2",
   "metadata": {},
   "source": [
    "Ahora queremos conocer los movimientos de R2 tanto de train como de test cuando alteramos los números de folds que usamos al trabajar con la validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad9f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizamos los cambios de R2 y alpha cuando variamos el n de folds (no tiene cambios, eso esta correcto?)\n",
    "\n",
    "cv_check_folds = []\n",
    "for i in range (2, 11):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "    kf = KFold(n_splits=i, shuffle=True, random_state=12)\n",
    "    lm_ridgeCV = linear_model.RidgeCV(alphas=[0.0001, 0.0005, 0.001, 0.005, 0.01,\\\n",
    "                                        0.05, 0.1, 1, 5, 10, 100], cv=kf) \n",
    "    reg_lineal_ridgeCV = lm_ridgeCV.fit(X_train, y_train)\n",
    "    score_ridgeCV_train = reg_lineal_ridgeCV.score(X_train, y_train)\n",
    "    score_ridgeCV_test = reg_lineal_ridgeCV.score(X_test, y_test)\n",
    "    best_alpha_ridgeCV = lm_ridgeCV.alpha_\n",
    "\n",
    "    cv_check_folds_1 = (i, score_ridgeCV_train, score_ridgeCV_test,best_alpha_ridgeCV)\n",
    "    cv_check_folds.append(cv_check_folds_1)\n",
    "\n",
    "df_array_folds = pd. DataFrame(cv_check_folds, columns = ['Folds','R2_train','R2_test','best_alpha'])\n",
    "df_array_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficos de los R2\n",
    "plt.plot(df_array_folds.Folds, df_array_folds.R2_train, c='r', label='R2_train'),\n",
    "plt.plot(df_array_folds.Folds, df_array_folds.R2_test, c='g', label='R2_test'),\n",
    "plt.xlabel('Folds')\n",
    "plt.ylabel('R2 values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#grafico de alpha\n",
    "plt.plot(df_array_folds.Folds, df_array_folds.best_alpha, c='b', label = 'alpha'),\n",
    "plt.xlabel('Folds'),\n",
    "plt.ylabel('Alpha values'),\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f4303",
   "metadata": {},
   "source": [
    "<a id=\"section_lineal_lasso\"></a> \n",
    "## 7. Regresión Lineal Lasso\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca63633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciamos el modelo de Lasso:\n",
    "lm_lassoCV =  linear_model.LassoCV(alphas=np.linspace(0.001, 100, num = 100), cv = kf, max_iter = 20000, tol=0.000001)\n",
    "\n",
    "#Entrenamos el modelo\n",
    "reg_lineal_lassoCV = lm_lassoCV.fit(X_train, y_train)\n",
    "\n",
    "#Guardamos el score\n",
    "score_lassoCV = reg_lineal_lassoCV.score(X_train, y_train)\n",
    "\n",
    "#Prediccion con el set de testeo:\n",
    "print('Score del modelo Lasso:', score_lassoCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa35da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cuál es el alpha óptimo de Lasso:\n",
    "best_alpha_lassoCV = lm_lassoCV.alpha_\n",
    "best_alpha_lassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = linear_model.Lasso(alpha = best_alpha_lassoCV, fit_intercept = True, max_iter = 20000)\n",
    "model_fit_lasso = model_lasso.fit(X_train, y_train)\n",
    "print('intercepto_l :', model_fit_lasso.intercept_.round(3))\n",
    "print('coeficientes_l :', model_fit_lasso.coef_.round(3))\n",
    "print('score_l :', model_fit_lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a303468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario para luego mostrar los datos en un dataframe:\n",
    "lasso_model = {'Modelo': \"Regresión Lineal Lasso\",\n",
    "                          'Alcance del modelo': 'Buenos Aires',\n",
    "                          'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                          'Cantidad de Observaciones': len(df_properatti),\n",
    "                          'R2_train': score_lassoCV,\n",
    "                          'R2_test': model_fit_lasso.score(X_test, y_test),\n",
    "                          'Intercepto': reg_lineal_lassoCV.intercept_,\n",
    "                          'Alpha del modelo': best_alpha_lassoCV}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6c67c",
   "metadata": {},
   "source": [
    "#### Chequeo Georeferencial\n",
    "Chequeamos geográficamente si hay relación entre los coeficientes de lasso = 0 y los p-value de los coeficientes de StatsModel (SM) mayores a 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la constante de los parámetros de sm, para comparar con los otros coeficientes:\n",
    "mod_pvalues = mod_pvalues.drop(mod_pvalues.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictio={'Col_X': X.columns,\n",
    "       'lasso_coef': model_fit_lasso.coef_,\n",
    "       'sm_pvalue': mod_pvalues,\n",
    "       'ridge_coef': model_fit_ridge.coef_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd655ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = pd.DataFrame (dictio)\n",
    "data_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56097dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coinciden los lasso = 0 con los p_values > 0.005? No, pero resulta útil visualizarlo gráficamente:\n",
    "p_v = 0.005\n",
    "\n",
    "data_params[\"coincidencias\"] = (data_params.lasso_coef == 0) & (data_params.sm_pvalue > p_v)\n",
    "\n",
    "\n",
    "print(f'Coef Lasso = 0:     {(data_params.lasso_coef == 0).sum()}  {np.round((data_params.lasso_coef == 0).sum()/data_params.shape[0]*100, 2)}%')\n",
    "print(f'P_values > 0.005:  {(data_params.sm_pvalue > p_v).sum()}  {np.round((data_params.sm_pvalue > p_v).sum()/data_params.shape[0]*100, 2)}%')\n",
    "print('total de datos:   ',   data_params.shape[0])\n",
    "print()\n",
    "print(f'{data_params[\"coincidencias\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43afbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_params.to_csv(path_or_buf='analisis.csv', sep=';', encoding = 'latin1', decimal= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca64b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este código es para ver sólo un valor por place involucrado en los coef. analizados.\n",
    "geo_properatti_2 = geo_properatti.drop_duplicates (subset = 'place_name')\n",
    "geo_properatti_params = data_params.merge(geo_properatti_2, left_on = 'Col_X', right_on='place_name', how = 'left')\n",
    "geo_properatti_params.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2860d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este código es para ver todos los valores del barrio involucrado, en los coef. analizados:\n",
    "# geo_properatti_params = data_params.merge(geo_properatti, left_on = 'Col_X', right_on='place_name', how = 'left')\n",
    "# geo_properatti_params.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc75b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_properatti_sm = geo_properatti_params[geo_properatti_params.sm_pvalue > p_v]\n",
    "geo_properatti_lasso = geo_properatti_params[geo_properatti_params.lasso_coef == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd45048",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_properatti_sm = gpd.GeoDataFrame(geo_properatti_sm)\n",
    "geo_properatti_lasso = gpd.GeoDataFrame(geo_properatti_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc499ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "departamentos_arg.plot(ax=ax, color='white', edgecolor='black')\n",
    "\n",
    "\n",
    "geo_properatti_sm.plot(color='red', ax=ax, zorder=5, markersize = 50, marker = 'x', label = 'p_value > 0.005')\n",
    "geo_properatti_lasso.plot(color='green', ax=ax, zorder=5, markersize = 20, marker = 'o', label = 'Lasso coef = 0')\n",
    "\n",
    "\n",
    "ax.set_title('Relacion geografica entre coeficientes con inconsistencias', \n",
    "             pad = 20, \n",
    "             fontdict={'fontsize':20, 'color': '#4873ab'})\n",
    "\n",
    "ax.set_xlabel('Longitud')\n",
    "ax.set_ylabel('Latitud')\n",
    "\n",
    "ax.set(xlim=(-59.1, -58.3), ylim=( -34.95, -34.25))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560ca24",
   "metadata": {},
   "source": [
    "<a id=\"section_elastic_net\"></a> \n",
    "## 8. Regresión Elastic Net\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37582e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciamos el modelo:\n",
    "lm_elastic_netCV =  linear_model.ElasticNetCV(l1_ratio= [.1, .5, .7, .9, .95, .99, 1], alphas = [0.0001, 0.001, 0.01, 1.0, 5, 10, 20, 50, 100, 1000], cv = kf)\n",
    "\n",
    "#Entrenamos el modelo:\n",
    "elastic_netCV = lm_elastic_netCV.fit(X_train, y_train)\n",
    "\n",
    "#Guardamos el score:\n",
    "score_EN_CV = elastic_netCV.score(X_train, y_train)\n",
    "\n",
    "#Predicción con el set de testeo:\n",
    "print('Score del modelo Elastic Net:', score_EN_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c53e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cuál es el alpha óptimo de Elastic Net:\n",
    "best_alpha_EN_CV = lm_elastic_netCV.alpha_\n",
    "best_alpha_EN_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_EN = linear_model.ElasticNet(l1_ratio= 0.5, alpha = best_alpha_EN_CV, fit_intercept = True, normalize = False)\n",
    "model_fit_EN = model_EN.fit(X_train, y_train)\n",
    "print(model_fit_EN.intercept_)\n",
    "print(model_fit_EN.coef_)\n",
    "print(model_fit_EN.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario para luego mostrar los datos en un dataframe:\n",
    "\n",
    "Elastic_Net_model = {'Modelo': \"Elastic Net\",\n",
    "                        'Alcance del modelo': 'Buenos Aires',\n",
    "                        'Variable Objetivo': 'ARS_to_USD_corregido',\n",
    "                        'Cantidad de Observaciones': len(df_properatti),\n",
    "                        'R2_train': score_EN_CV,\n",
    "                        'R2_test': model_fit_EN.score(X_test, y_test),\n",
    "                        'Intercepto': elastic_netCV.intercept_,\n",
    "                        'Alpha del modelo': elastic_netCV.alpha_,\n",
    "                        'l1_ratio': elastic_netCV.l1_ratio_}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c974707",
   "metadata": {},
   "source": [
    "<a id=\"section_conclusiones\"></a> \n",
    "## 9. Conclusiones\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c22162",
   "metadata": {},
   "source": [
    "Para poder presentar las conclusiones de nuestro análisis, lo primero que haremos es reunir los resultados de cada modelo, guardados en diccionarios por separado, para tener todo en un único Dataframe y compararlos más cómodamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ac315",
   "metadata": {},
   "source": [
    "<a id=\"section_9.1\"></a> \n",
    "### 9.1 Comparación Numérica de los Distintos Modelos\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad83a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista de todos los diccionarios:\n",
    "diccionarios = [\n",
    "reg_lineal_simple_model_m2,\n",
    "reg_lineal_simple_model_amb,\n",
    "reg_lineal_simple_model_est,\n",
    "reg_lineal_simple_model_cochera,\n",
    "reg_lineal_simple_model_scm2,\n",
    "reg_multiple_sin_reg_model,\n",
    "ridge_model,                \n",
    "lasso_model,\n",
    "Elastic_Net_model\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd258191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos el Dataframe con toda la info:\n",
    "resultados = pd.DataFrame(diccionarios)\n",
    "\n",
    "#Ordenamos las columnas en el siguiente orden para verlo mas comodamente:\n",
    "resultados = resultados[['Modelo','Alcance del modelo','Variable Objetivo','Cantidad de Observaciones','R2_train','R2_test','Intercepto','Alpha del modelo','l1_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos que se haya generado correctamente:\n",
    "resultados\n",
    "#Faltaría una línea mas en el Dataframe con los resultados de la Regresión Lineal Múltiple sin Estandarización de variables y sin variables Dummies,\n",
    "# que lo agregaré cuando arreglemos lo de la columna 'ambientes'. En ese modelo usamos solo a \"surface_total_in_m2\" y a \"ambientes\" como\n",
    "# variables predictoras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasamos los datos a un archivo para guardarlo:\n",
    "\n",
    "#resultados.to_csv(path_or_buf='Resultados.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcd0d7",
   "metadata": {},
   "source": [
    "<br>\n",
    "A continuación mostraremos en un Groupby y una Pivot Table cuál es el mejor modelo basado en el R2:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9eeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si lo queremos ver como Groupby:\n",
    "resultados_R2= resultados.groupby('Modelo').mean()['R2_train'].sort_values(ascending=False).round(4)\n",
    "resultados_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32866e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si lo queremos ver como Pivot Table:\n",
    "resultados.pivot_table(['R2_train','R2_test'],['Modelo'],aggfunc='mean').round(4).sort_values(by='R2_test',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efde0e",
   "metadata": {},
   "source": [
    "<a id=\"section_9.2\"></a> \n",
    "### 9.2 Comparación Gráfica de los Distintos Modelos\n",
    "\n",
    "[volver a TOC](#section_TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffbaa6",
   "metadata": {},
   "source": [
    "A continuación mostraremos en un gráfico de Barras cuál es el mejor modelo, basado en el R2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el gráfico:\n",
    "\n",
    "y = np.log(resultados_R2)\n",
    "my_cmap = plt.get_cmap('bwr') #buscar distintos colores en: https://matplotlib.org/stable/tutorials/colors/colormaps.html#sphx-glr-tutorials-colors-colormaps-py\n",
    "rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "colors = ['black', 'maroon', 'darkgreen', 'navy', 'grey', 'orangered', 'limegreen', 'royalblue']\n",
    "chart = resultados_R2.sort_values().plot.barh(figsize=(14,5),fontsize=15, color = my_cmap(rescale(y)))\n",
    "chart.set_title(\"R2 según Tipo de Modelo\", fontsize=25)\n",
    "chart.set_ylabel(\"Modelo\", fontsize=25)\n",
    "chart.set_xlabel(\"R2\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399600d",
   "metadata": {},
   "source": [
    "Podemos arribar a la conclusión de que los modelos que mejor R² obtuvieron fueron los de Regresión Lineal Ridge, Regresión Lineal Lasso y Elastic Net, de mayor a menor R², respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46394437",
   "metadata": {},
   "source": [
    "#### Los coeficientes que más influencia tienen en la Regresión lineal son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params.lasso_coef = abs(data_params.lasso_coef)\n",
    "Coef_importantes = data_params.sort_values('lasso_coef',ascending=False).iloc[0:41]\n",
    "\n",
    "y = np.log(Coef_importantes.lasso_coef)\n",
    "my_cmap = plt.get_cmap('Wistia')\n",
    "rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.bar(Coef_importantes.Col_X, Coef_importantes.lasso_coef, color = my_cmap(rescale(y)))\n",
    "plt.xticks(Coef_importantes.Col_X, Coef_importantes.Col_X, rotation=90)\n",
    "plt.title(\"Los 40 coeficientes más influyentes en el modelo Lasso\", fontsize=25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793cddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params.lasso_coef = abs(data_params.lasso_coef)\n",
    "Coef_importantes = data_params.sort_values('lasso_coef',ascending=True).iloc[0:96]\n",
    "\n",
    "y = (Coef_importantes.lasso_coef)\n",
    "my_cmap = plt.get_cmap('winter')\n",
    "rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "plt.figure(figsize=(23,4))\n",
    "plt.bar(Coef_importantes.Col_X, Coef_importantes.lasso_coef, color = my_cmap(rescale(y)))\n",
    "plt.xticks(Coef_importantes.Col_X, Coef_importantes.Col_X, rotation=90)\n",
    "plt.title(\"Los 95 coeficientes menos influyentes en el modelo Lasso\", fontsize=25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params.ridge_coef = abs(data_params.ridge_coef)\n",
    "Coef_importantes = data_params.sort_values('ridge_coef',ascending=False).iloc[0:41]\n",
    "\n",
    "y = np.log(Coef_importantes.ridge_coef)\n",
    "my_cmap = plt.get_cmap('Wistia')\n",
    "rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.bar(Coef_importantes.Col_X, Coef_importantes.ridge_coef, color = my_cmap(rescale(y)))\n",
    "plt.xticks(Coef_importantes.Col_X, Coef_importantes.Col_X, rotation=90)\n",
    "plt.title(\"Los 40 coeficientes más influyentes en el modelo Ridge\", fontsize=25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27961b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params.ridge_coef = abs(data_params.ridge_coef)\n",
    "Coef_importantes = data_params.sort_values('ridge_coef',ascending=True).iloc[0:41]\n",
    "\n",
    "y = (Coef_importantes.ridge_coef)\n",
    "my_cmap = plt.get_cmap('winter')\n",
    "rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.bar(Coef_importantes.Col_X, Coef_importantes.ridge_coef, color = my_cmap(rescale(y)))\n",
    "plt.xticks(Coef_importantes.Col_X, Coef_importantes.Col_X, rotation=90)\n",
    "plt.title(\"Los 40 coeficientes menos influyentes en el modelo Ridge\", fontsize=25)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
